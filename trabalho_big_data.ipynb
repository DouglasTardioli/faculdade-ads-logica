{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcZUoQJc9Zbs"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi projetado para guiar os alunos na realização das práticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execução das atividades.\n",
        "\n",
        "Seu trabalho começará na célula 5. Execute as 4 primeiras células para iniciar a atividade.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-kcj56B9Zbt"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, você fará o download do dataset necessário para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "teEzlVcD9Zbu",
        "outputId": "e3b63642-6ba6-4018-96d4-0f2490b59c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-31 11:28:41--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M  53.7MB/s    in 0.9s    \n",
            "\n",
            "2025-03-31 11:28:47 (53.7 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcGDDx3s9Zbu"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necessárias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V9Ul1iz_9Zbv",
        "outputId": "687fb6a7-4a78-45d1-b913-726ca52da6f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8-2ShdF9Zbv"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession é o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "n9SdADVg9Zbv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "RU = \"4568725\"\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbu-Ysc59Zbw"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark\n",
        "\n",
        "Não altere este código e use o dataframe imdb_df criado aqui em todo o seu trabalho. A criação de um dataframe diferente deste poderá causar erros na coluna sentiment e isso refletirá em erros de resposta das questões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n9ssfqB59Zbw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "schema = StructType([\n",
        "  StructField(\"id\", StringType(), True),\n",
        "  StructField(\"text_en\", StringType(), True),\n",
        "  StructField(\"text_pt\", StringType(), True),\n",
        "  StructField(\"sentiment\", StringType(), True),\n",
        "])\n",
        "\n",
        "\n",
        "imdb_df: DataFrame = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJq36HPb9Zbw"
      },
      "source": [
        "# Questão 1\n",
        "\n",
        "Nesta questão, você irá calcular a soma dos IDs para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBf8th49Zbw"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "Nn3wg7YY9Zbx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def filter_negative_reviews(data: DataFrame) -> DataFrame:\n",
        "\n",
        "    # (RU)\n",
        "    RU = \"4568725\"\n",
        "\n",
        "    # Exibe o RU no console\n",
        "    print(f\"RU: {RU}\")\n",
        "\n",
        "    # Filtra as linhas onde a coluna 'sentiment' é igual a 'neg' (avaliações negativas)\n",
        "    return data.filter(data[\"sentiment\"] == \"neg\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QxLwU3gRJMSG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpaxxKYq9Zbx"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A função reduce irá somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Sywtnaog9Zbx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "def sum_negative_ids(reviews: DataFrame) -> DataFrame:\n",
        "\n",
        "    # Converte a coluna 'id' para o tipo inteiro, caso não esteja nesse formato\n",
        "    reviews = reviews.withColumn(\"id\", col(\"id\").cast(\"int\"))\n",
        "\n",
        "    # Calcula a soma da coluna 'id' e retorna o resultado como DataFrame\n",
        "    return reviews.select(sum(\"id\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYIJhk8B9Zbx"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "Aqui, você aplicará as funções de map e reduce ao dataframe Spark para calcular os resultados. Não se esqueça de usar o método `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "NQ2UcU7n9Zby",
        "outputId": "5beed21a-3004-498d-b218-4336d0b01a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RU: 4568725\n",
            "Soma de IDs com reviews negativas: 459568555\n"
          ]
        }
      ],
      "source": [
        "# Filtra apenas as avaliações negativas do DataFrame imdb_df\n",
        "negative_reviews = filter_negative_reviews(imdb_df)\n",
        "\n",
        "# Calcula a soma dos IDs das avaliações negativas e coleta o valor resultante\n",
        "sum_of_negative_ids = sum_negative_ids(negative_reviews).collect()[0][0]\n",
        "\n",
        "# Exibe o resultado da soma dos IDs no console\n",
        "print(f\"Soma de IDs com reviews negativas: {sum_of_negative_ids}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd0Bvfzl9Zby"
      },
      "source": [
        "# Questão 2:\n",
        "\n",
        "Nesta questão, você irá calcular a diferença no número total de palavras entre textos negativos em português e inglês.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') é 'neg'.\n",
        "- Subtrair o total de palavras em inglês do total em português."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54WD7vCG9Zby"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A função map irá transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para então descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o método .split() e não .split(\" \") de string para separar as palavras em uma lista ou use a função split(coluna de texto, regex) do pyspark com o regex igual à \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "A2uvQLz_9Zby"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "def map_sentiment_to_word_count(data: DataFrame) -> DataFrame:\n",
        "\n",
        "    # Número de Registro do Usuário (RU)\n",
        "    RU = \"4568725\"\n",
        "\n",
        "    # Exibe o RU no console\n",
        "    print(f\"RU: {RU}\")\n",
        "\n",
        "    # Retorna um novo DataFrame com:\n",
        "    # - A coluna 'sentiment'\n",
        "    # - A contagem de palavras na coluna 'text_en'\n",
        "    # - A contagem de palavras na coluna 'text_pt'\n",
        "    return data.select(\n",
        "        col(\"sentiment\"),\n",
        "        size(split(col(\"text_en\"), \"\\\\s+\")).alias(\"text_en_word_count\"),  # Conta palavras no texto em inglês\n",
        "        size(split(col(\"text_pt\"), \"\\\\s+\")).alias(\"text_pt_word_count\")   # Conta palavras no texto em português\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDBmaoYU9Zby"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\" (dependerá de como você optou por fazer sua função map2).\n",
        "\n",
        "A função reduce irá somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Q1jcxfOi9Zby"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "def reduce_word_count_by_sentiment(sentiment_word_counts: DataFrame) -> DataFrame:\n",
        "    # Agrupa os dados pelo campo 'sentiment' e soma as contagens de palavras\n",
        "    return sentiment_word_counts.groupBy(\"sentiment\").agg(\n",
        "        # Soma a contagem de palavras dos textos em inglês e renomeia a coluna\n",
        "        sum(\"text_en_word_count\").alias(\"total_text_en_words\"),\n",
        "\n",
        "        # Soma a contagem de palavras dos textos em português e renomeia a coluna\n",
        "        sum(\"text_pt_word_count\").alias(\"total_text_pt_words\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q7lt83p9Zbz"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "i5m6zla_9Zbz",
        "outputId": "9ea24e26-6afe-4a4a-d478-1160c909c9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RU: 4568725\n",
            "RU: 4568725\n",
            "Diferença entre Texto em Inglês e em Português:\n",
            "                    54976\n"
          ]
        }
      ],
      "source": [
        "# Filtra apenas as avaliações negativas do DataFrame imdb_df\n",
        "negative_data = filter_negative_reviews(imdb_df)\n",
        "\n",
        "# Mapeia os sentimentos para a contagem de palavras nos textos em inglês e português\n",
        "sentiment_word_counts = map_sentiment_to_word_count(negative_data)\n",
        "\n",
        "# Reduz os dados somando o total de palavras nos textos em inglês e português, agrupados por sentiment\n",
        "total_word_counts = reduce_word_count_by_sentiment(sentiment_word_counts)\n",
        "\n",
        "# Coleta o primeiro resultado do DataFrame como uma linha de dados\n",
        "result = total_word_counts.collect()[0]\n",
        "\n",
        "# Calcula a diferença entre a contagem de palavras nos textos em português e em inglês\n",
        "word_count_difference = result[2] - result[1]\n",
        "\n",
        "# Exibe a diferença calculada no console\n",
        "print(f\"Diferença entre Texto em Inglês e em Português:\")\n",
        "print(f\"                    {word_count_difference}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}